% !Rnw root = ../reports/dami_qame_3_report.Rnw
\SweaveOpts{prefix.string=EPFL}

<<label=performance, fig=TRUE, include=FALSE>>=
tbl <- read.table("../eric/data.txt", header=T)

# Barplot of runtimes of our algoritm
text(x=barplot(tbl[,2], main="Runtime of our apriori-like implementation",
        ylab="seconds", xlab="minsup", names=tbl[,1], ylim=c(0,550)),
     y=tbl[,2], label=format(tbl[,2]), po=3)
@

<<label=totalpatterns, fig=TRUE, include=FALSE>>=
#Total number of items found by algorithm
text(x=barplot(tbl[,3], main="Effect of minsup on number of patterns found",
            ylab="Total number of frequent patterns", xlab="minsup",
            names=tbl[,1], ylim=c(0,7000)),
        y=tbl[,3], label=format(tbl[,3]), po=3)
@

<<label=candidatevsfrequent, fig=TRUE, include=FALSE>>=
patterns <-  as.matrix(tbl[,c(4,6,8,10,12,14,16)])
candidates <- as.matrix(tbl[,c(5,7,9,11,13,15)])

layout(rbind(1,2), heights=c(7,1))

# 0.02
plot(2:7, candidates[5,], main="Candidates vs. frequent patterns",
     xlim=c(1,7), log="y", ylab="Number of patterns", xlab="Pattern length",
     t="b", col="red")
lines(1:7, patterns[5,], col="red", type="o", pch=22, lty=2)

# 0.03
lines(2:7, candidates[4,], col="blue", t="b")
lines(1:7, patterns[4,], col="blue", t="o", pch=22, lty=2)

# 0.05
lines(2:7, candidates[3,], col="purple", t="b")
lines(1:7, patterns[3,], col="purple", t="o", pch=22, lty=2)

par(mar=c(0, 0, 0, 0))
plot.new()

legend("center", "groups", c("0.02 candidates", "0.02 frequent",
                   "0.03 candidates", "0.03 patterns",
                   "0.05 candidates", "0.05 frequent"),
       lty=c(1,2,1,2,1,2), lwd=c(1,1,1,1,1,1),
       col=c("red", "red", "blue", "blue", "purple", "purple"), ncol=3)
@

\begin{multicols}{2}

\subsection{Apriori-like algorithm in Python}

We implemented the apriori-like algorithm in Python according to the
specification introduced earlier, with support for maxgap and mingap
parameters.

The performance of the algorithm and the total number of frequent patterns
discovered, using different values of minsup, has been illustrated in
Figure~\ref{fig:performance} and Figure~\ref{fig:totalpatterns}.
The number of candidate vs. frequent patterns generated by different choices of
minsup has been illustrated in Figure~\ref{fig:candidatevsfrequent}.

\begin{figure}[H]
    \includegraphics[width=9cm]{EPFL-performance}
    \caption{Performance of our implementation.}
    \label{fig:performance}
\end{figure}
\begin{figure}[H]
    \includegraphics[width=9cm]{EPFL-totalpatterns}
    \caption{Number of frequent patterns discovered.}
    \label{fig:totalpatterns}
\end{figure}

\end{multicols}
\begin{figure}
    \includegraphics[scale=1.0]{EPFL-candidatevsfrequent}
    \caption{Number of candidates vs. frequent patterns generated with
        different choices of minsup.}
    \label{fig:candidatevsfrequent}
\end{figure}


\pagebreak
\subsection{maxgap and mingap}
We benchmarked the Python implementation with different mingap and maxgap
values. The following is a listing of our results, indicating, in order:
parameter selections (maxgap, mingap), total run time (runtime), total number
of patterns found (total) and counts for different lengths of patterns
(n-patterns):

\begingroup
    \fontsize{10pt}{12pt}\selectfont
    \verbatiminput{../eric/data2.txt}
\endgroup

From the above results we can see that tightening the constraints really does
reduce the number of returned patterns. This in turn, reduces the number of
candidates generated, meaning that less occurrences need to be checked, which
leads to faster performance (since occurrence checking is a bottleneck of sorts).

With mingap=9 no more 2-patterns are found, only 1-patterns are left. In the
last line, $mingap > maxgap$ which actually can't be satisfied with patterns of
length 2 or greater.

\begin{multicols}{2}
\subsection{Occurrence checking}
The actual skeleton of the apriori-like algorithm isn't hard to implement. Even
the candidate generation is moderately easy, at least in our case where we
aren't considering patterns consisting of sequences of sets. The real problem
lies with checking whether a pattern occurs in a data sequence or not.
Conveniently, the book has no explanation on how this should be done.

Our approach is the following. Suppose we have a pattern and we wish to check
whether it occurs in a specific data sequence. First for all items (events) in
the pattern, figure out all the semesters to which it belongs. Conceptually
build a "tree" of the semesters to which each item belongs to and convert the
semesters to their respective time units. This is depicted in
Figure~\ref{fig:algorithm_logic}.

After this "tree" has been built, find all paths from top to bottom using a
state machine -based method. Of course maxgap, mingap and maxspan set some
additional constraints on what kind of paths are allowed. These are,
however, relatively easy to check once the main logic has been implemented.

\begin{figure}[H]
    \includegraphics[scale=0.08]{../eric/algorithm_logic.pdf}
    \caption{Say we have a pattern \texttt{<a, e, g, k>} and a data sequence
        \texttt{\{e,f,h\}\{a,e\}\{d,e,l,m\}\{a,g\}\{e\} \ \{g,j,u\}\{a,k,q\}\{g,k,a\}}.
        The above figure represents where each item of the patterns occurs
        (by index aka time unit). All strictly increasing paths from top to
        bottom are valid occurrences of the pattern.
    }
    \label{fig:algorithm_logic}
\end{figure}


Our Python implementation spends most of its time checking occurrences. For
example in Figure~\ref{fig:performance} with minsup $0.02$ a total of 381
seconds or around 82\% was spent merely checking for occurrences. This is
indeed slow, and trying a more effective strategy or programming language could
speed up the mining process a lot.
\end{multicols}
