% !Rnw root = ../reports/dami_qame_4_slides.Rnw


<<negatively_correlated_comput>>=
source("../arto/negatively_correlated_comput.R")
@


\begin{frame}
\begin{block}{{\Huge\centerline{Negatively correlated patterns}}}
\bigskip
\begin{itemize}
\item negatively correlated itemsets
\item negatively correlated association rules
\end{itemize}
\end{block}
\end{frame}


\begin{frame}
\frametitle{Negatively correlated itemset}
\begin{itemize}
\item \textit{Negatively correlated itemset} is an itemset that is (considerably) less frequent than what we would estimate by assuming that all the items are statistically independent.
\bigskip
\item{ \textbf{Motivation:} Identification of \textit{competing items}, i.e. items that can be substituted for one another. E.g:
  \begin{itemize}
  \item tea vs. coffee
  \item butter vs. margarine
  \item desktop vs. laptop computers
  \end{itemize}
}
\bigskip
\item \textbf{Mathematically:} Itemset $X$ consisting of items $x_1,\ldots{},x_k$ is negatively correlated if  $P(X) < \prod{}_{j=1}^{k}P(x_j)$
\item What, Why?!? Let's start on the left hand side, $P(X)$, ...
% \item Note that right hand side of the inequality $\sum{}_{j=1}^{k}P(x_j)$ , represents an estimate of the probability that all the items in $X$ are statistically independent.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Relative frequency = Support = (marginal) Probability}
\begin{itemize}
\item $b=$ bread =  \textit{item}
\item $\sigma(b)=$ \textit{support count} = "In how many transactions $b$ occurs"
\item $N=|T|=$ "number of transactions"
\end{itemize}
\begin{eqnarray*}
\frac{\sigma(b)}{N} & = & f(b)=\mbox{"\textit{relative frequency}"}\\
 & = & s(b)=\mbox{"\textit{support}"}\\
 & = & P(b)=\mbox{"\textit{(marginal) probability} of $b$ happening"}
\end{eqnarray*}
\end{frame}


\begin{frame}
\frametitle{Example}
\begin{columns}[c]
\column{.40\textwidth}
\centering
b=bread, c=coffee
<<negatively_correlated_comput_out1>>=
n <- nrow(negap2$tab21)
print(xtable(negap2$tab21[,1:2], digits=0),hline.after=c(-1,0,n-1,n))
@
\column{.60\textwidth}
\begin{eqnarray*}
P(b) & = & \frac{\sigma(b)}{N} = \frac{4}{6} = 0.67\\
P(c) & = & \frac{\sigma(c)}{N} = \frac{3}{6} = 0.50
\end{eqnarray*}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Joint probability of an itemset}
\begin{itemize}
\item $b,c$ are items
\item can be thought to be singleton sets $\{b\}, \{c\}$
\item so $b \cup c = \{b,c\}$ = itemset $X$
\bigskip
\end{itemize}
\[
\frac{\sigma(b\cup c)}{N} = \frac{\sigma(\{b,c\})}{N} = P(b,c) = P(X) 
\]
\centerline{="\textit{joint probability} of both $b$ and $c$
happening at the same time"}
\end{frame}


\begin{frame}
\frametitle{Example}
\begin{columns}[c]
\column{.40\textwidth}
\centering
b=bread, c=coffee
<<negatively_correlated_comput_out1>>=
n <- nrow(negap2$tab21)
print(xtable(negap2$tab21, digits=0),hline.after=c(-1,0,n-1,n))
@
\column{.60\textwidth}
\begin{eqnarray*}
P(b) & = & \frac{\sigma(b)}{N} = \frac{4}{6} = 0.67\\
P(c) & = & \frac{\sigma(c)}{N} = \frac{3}{6} = 0.50\\
P(b,c) & = & \frac{\sigma(\{b,c\})}{N} = \frac{2}{6} = 0.33
\end{eqnarray*}
\end{columns}
\end{frame}



\begin{frame}
\frametitle{Negatively correlated itemset - part deux}
\begin{itemize}
\item Ok. So we now understand the left hand side, $P(X)$, of the inequality:
\bigskip
\item Itemset $X$ consisting of items $x_1,\ldots{},x_k$ is negatively correlated if  $P(X) < \prod{}_{j=1}^{k}P(x_j)$
\bigskip
\item but what about the right hand side, how is that related to statistical independence... 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Statistical independence - joint probabilites}
\begin{itemize}
\item Axiom: $b$ and $c$ are independent if and only if their joint probability equals the product of their probabilities:
\[
b\bot c \Longleftrightarrow P(b,c)=P(b)P(c)
\]
\item let's first just do an example and later understand this with conditional probabilites...
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Example}
\begin{columns}[c]
\column{.40\textwidth}
\centering
b=bread, c=coffee
<<negatively_correlated_comput_out1>>=
n <- nrow(negap2$tab21)
print(xtable(negap2$tab21, digits=0),hline.after=c(-1,0,n-1,n))
@
\column{.60\textwidth}
\begin{eqnarray*}
P(b) & = & \frac{\sigma(b)}{N} = \frac{4}{6} = 0.67\\
P(c) & = & \frac{\sigma(c)}{N} = \frac{3}{6} = 0.50\\
P(b,c) & = & \frac{\sigma(\{b,c\})}{N} = \frac{2}{6} = 0.33\\
P(b)P(c) & = &  \frac{4}{6} * \frac{3}{6} = \frac{12}{36} = \frac{1}{3} = 0.33\\
\frac{P(b,c)}{P(b)P(c)} & = & \frac{2/6}{1/3} = \frac{0.33}{0.33} = 1.00
\end{eqnarray*}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Statistical independence - conditional probabilites}
"If we know that b happened, how likely is c to happen"
 = \textit{conditional probability} of $b$ given $c$ = 
\[
P(b|c)=\frac{P(b,c)}{P(c)}
\]
\begin{itemize}
\item independence $b\bot c$ means that the occurrence of one does not affect the probability of the other, i.e. $P(b)=P(b|c)$ and $P(c)=P(c|b)$.
\item formally:
\begin{eqnarray*}
P(b)P(c) & = & P(b,c) \mbox{ \qquad from axiom}\\
\Longleftrightarrow P(b) & = & \frac{P(b,c)}{P(c)}\\
\Longleftrightarrow P(b) & = & P(b|c)
\end{eqnarray*}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Example}
\begin{columns}[c]
\column{.40\textwidth}
\centering
b=bread, c=coffee
<<negatively_correlated_comput_out1>>=
n <- nrow(negap2$tab21)
print(xtable(negap2$tab21, digits=0),hline.after=c(-1,0,n-1,n))
@
\column{.60\textwidth}
\begin{eqnarray*}
P(b) & = & \frac{\sigma(b)}{N} = \frac{4}{6} = 0.67\\
P(b|c) & = & \frac{P(b,c)}{P(c)} = \frac{2/6}{3/6} = \frac{2}{3} = 0.67\\
P(c) & = & \frac{\sigma(c)}{N} = \frac{3}{6} = 0.50\\
P(c|b) & = & \frac{P(b,c)}{P(b)} = \frac{2/6}{4/6} = \frac{2}{4} = 0.50\\
P(b,c) & = & \frac{\sigma(\{b,c\})}{N} = \frac{2}{6} = 0.33\\
P(b)P(c) & = & \frac{4}{6} * \frac{3}{6} = \frac{12}{36} = \frac{1}{3} = 0.33\\
\frac{P(b,c)}{P(b)P(c)} & = & \frac{2/6}{1/3} = \frac{0.33}{0.33} = 1.00
\end{eqnarray*}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Creating negatively correlated itemsets}
\begin{itemize}
\item How do we have to change the data to make itemset $\{b,c\}$ negatively correlated?
\item examples... 
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\begin{columns}[c]
\column{.50\textwidth}
\centering
Uncorrelated (independent)
<<negatively_correlated_comput_out1>>=
negap2$outme2()
@
\column{.50\textwidth}
\centering
Negatively correlated
<<negatively_correlated_comput_out2>>=
negap2$outme3()
@
\end{columns}
\end{frame}


\begin{frame}[fragile]
\begin{columns}[c]
\column{.50\textwidth}
\centering
Uncorrelated (independent)
<<negatively_correlated_comput_out3>>=
negap2$outme2()
@
\column{.50\textwidth}
\centering
Positively correlated
<<negatively_correlated_comput_out4>>=
negap2$outme1()
@
\end{columns}
\end{frame}


\begin{frame}
\frametitle{negatively correlated association rule}
\begin{itemize}
\item Two ways to define whether Rule $L\rightarrow R$ is \textit{negatively correlated association rule}:
\medskip
\item 1. way: $P(L,R) < P(L)P(R)$
\item 2. way: $P(L,R) < \prod_{i}P(l_i)\prod_{j}P(r_j)$
\medskip
\item Let's see why first way is usually preferable...
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Rule confidence = Conditional probability}
\begin{itemize}
\item $L,R$ are itemsets
\item $L\rightarrow R$ is an association rule
\bigskip
\item $\sigma(L\cup R) = P(L,R) =$ "\textit{joint probability} of both $L$ and $R$ happening at the same time"
\end{itemize}
\bigskip
\begin{eqnarray*}
\frac{\sigma(L\cup R)}{\sigma(L)} & = & c(L\rightarrow R)= \mbox{"\textit{confidence} of a rule \ensuremath{L\rightarrow R}"}\\
 & = & \\
\frac{P(L,R)}{P(L)} & = & P(R|L)= \mbox{"\textit{conditional probability} of \ensuremath{R} given \ensuremath{L}"} \\
 & = & \mbox{"If we know that \ensuremath{L} happened, how likely is \ensuremath{R} to happen"}
\end{eqnarray*}
\end{frame}


\begin{frame}
\frametitle{Rule lift < 1 indicates negative correlation}
Lift of a rule is:
\[
\frac{P(L|R)}{P(L)} = \frac{P(R,L)}{P(R)P(L)}
\]

\bigskip{}
Therefore lift < 1 indicates negative correlation:
\[
P(R,L)<P(R)P(L)\Longleftrightarrow\frac{P(R,L)}{P(R)P(L)} <  1
\]

\bigskip
$\Longrightarrow$ first way has a meaningful probabilistic interpretation \\

\bigskip
And let's examine an example where items in $L$ are positively correlated...

\end{frame}

\begin{frame}
\begin{columns}[c]
\column{.40\textwidth}
\centering
Rule: $L\rightarrow R$ \\ \bigskip
$L=\{b,c\}, R=\{d\}$
<<negatively_correlated_comput_out101>>=
n <- nrow(negap2$tab101)
print(xtable(negap2$tab101, digits=0),hline.after=c(-1,0,n-1,n))
@
\column{0.60\textwidth}
\begin{eqnarray*}
P(L) & = & P(\{b,c\})=3/6=0.50\\
P(R) & = & P(\{d\})=3/6=0.50\\
P(L,R) & = & P(\{b,c,d\})=1/6=0.17\\
P(L)P(R) & = & (3/6)*(3/6)=1/4=0.25\\
\\
\frac{P(L,R)}{P(L)P(R)} & = & \frac{1/6}{1/4}=\frac{4}{6}=0.67<1
\end{eqnarray*}
With way 1 we claim negative correlation
\end{columns}
\bigskip
But with way 2, we claim independence:
\[
P(b)P(c)P(d) = \frac{4}{6}*\frac{3}{6}*\frac{3}{6}=\frac{1}{6}=0.17
\]
\end{frame}

